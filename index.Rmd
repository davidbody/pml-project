---
title: "Course Project"
author: "David W. Body"
date: "August 20, 2015"
output: html_document
---

## Practical Machine Learning

This is my course project for the [Practical Machine Learning](https://www.coursera.org/course/predmachlearn) course from Coursera and the Johns Hopkins Bloomberg School of Public Health.

## Executive Summary

This project analyzes Human Activity Recognition data from a study of weight lifting exercise to see if we can predict the manner in which participants performed a weight lifting exercise based on sensor data. Using a Random Forest model, we acheive a prediction accuracy of about 99%.

## Background

Details and data are available at [groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).

Each of six participants performed one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions. In the first (A), each participant performed the curls correctly. In each of the others, the participants did the curls incorrectly: throwing the elbows (B), lifting the dumbbells only halfway (C), lowering the dumbbells only halfway (D), or throwing the hips (E).

The participants were outfitted with sensors on their arms, forearms, and waists, and sensors were also attached to the dumbbells. The goal of this project is to see how well we can predict the manner in which the exercise was performed using the sensor data.

For more information and an analysis of the data, see Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. **Qualitative Activity Recognition of Weight Lifting Exercises**. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## Getting the Data

We download the data from location specified in the project assignment. We also grab the test data for the second part of the project assignment.

```{r, echo=FALSE}
setwd("~/study/practical_machine_learning/project")
```

```{r}
if (!file.exists("./data/pml-training.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                  destfile="./data/pml-training.csv",
                  method="libcurl")
}

if (!file.exists("./data/pml-testing.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                  destfile="./data/pml-testing.csv",
                  method="libcurl")
}
```

## Exploring and Cleaning the Data

We start by reading the data into a data frame and looking at it.

```{r}
har_data <- read.csv("./data/pml-training.csv", stringsAsFactors = FALSE)
str(har_data)
dim(har_data)
```

There is a lot of data here!

Several variables contain many NA or blank values. I decided to discard these variables.

After some exploratory analysis and reading the Velloso, et al. article cited above, I decided to use variables whose names contain the strings "arm", "forearm", "belt", or "dumbbell".

There is a time-series aspect to this data that might be important, and the data contains timestamp variables. However, I decided to see if I could produce a model capable of making accurate predictions ignoring the temporal aspect of the data. (I did this partly because we haven't covered time-series analysis in this course or any of the other courses leading up to it in the Data Science Specialization.) As we'll see, it turns out that we can get a pretty accurate predictive model without any time-series analysis for this data.

```{r}
any_missing_or_blank <- sapply(har_data, function (x) any(is.na(x) | x == ""))
vars_with_data <- names(har_data[, !any_missing_or_blank])
predictors <- vars_with_data[grep("(fore)?arm|belt|dumbbell", vars_with_data)]
predictors
```

The outcome we want to predict is in the variable `classe`, so we create a data frame containing `classe` and our predictors.

```{r, message=FALSE, results='hide'}
library(dplyr)

vars_to_include <- c("classe", predictors)

selected_har_data <- select(har_data, one_of(vars_to_include))
selected_har_data$classe <- as.factor(selected_har_data$classe)
```

## Training the Model

In order to make our results reproducible, we set an arbitrary seed for R's random number generator.

```{r}
set.seed(1337)
```

We start by dividing our data in to training and testing sets. Because we have so much data, we'll use 60% of the data for training and 40% for testing.

```{r, message=FALSE}
library(caret)
```

```{r}
inTrain <- createDataPartition(y = selected_har_data$classe, p = 0.6, list = FALSE)
training <- selected_har_data[inTrain, ]
testing <- selected_har_data[-inTrain, ]

dim(training)
dim(testing)
```

To speed things up, we parallelize training the model across 6 CPU cores.

```{r, message=FALSE}
library(doParallel)
registerDoParallel(cores = 6)
```

We train a Random Forest model using 10-fold cross-validation and 250 trees in each forest.

```{r, message=FALSE}
modFit <- train(classe ~ .,
                data = training,
                method = "rf",
                trControl = trainControl(number = 10),
                ntree = 250)
```

Let's take a look at the final model produced by the random forest training.

```{r}
modFit$finalModel
```

The out-of-bag estimate of the error rate from the 10-fold cross-validation during training is about 1%, which is quite good. This is an estimate of what we might expect our out-of-sample error rate to be on other data not used to train the model.

## Out of Sample Error

Now we want to see how well our model performs on data that wasn't used to produce the model. We do this by generating predictions of `classe` on the testing data that we set aside earlier and comparing these predictions to the actual `classe` values in the testing data.

```{r}
predictions <- predict(modFit, newdata = testing)
confusionMatrix(data = predictions, testing$classe)
```

The confusion matrix shows us a table of our predictions versus the actual values of `classe` in the testing data, and allows us to easily compute the accuracy of our model's predictions on this data.

<p align="center" style="font-size: large">
$accuracy = \frac{number\ correct}{number\ correct + number\ incorrect}$
<p>

The numbers on the diagonal of the confusion matrix are our correct predictions, and the off-diagonal numbers are our incorrect predictions.

The output above conveniently includes a calculation of accuracy (about 99%) as well as a 95% confidence interval for accuracy.

The out-of-sample error rate is simply

<p align="center" style="font-size: large">
$error\ rate = 1 - accuracy \approx 1\%$
<p>

These results confirm that we can have a high level of confidence that our out-of-sample error rate will be close to 1%.

## Model Analysis

Using `varImp`, which we learned about on Quiz 3, we can easily see which variables are most important in our model.

```{r}
varImp(modFit)
```

To gain some insights into our model, we can plot combinations of the top several variables against each other, and color the points based on our outcome variable.

```{r}
qplot(roll_belt, yaw_belt, color = classe, data = training)
qplot(roll_belt, magnet_dumbbell_z, color = classe, data = training)
qplot(yaw_belt, magnet_dumbbell_z, color = classe, data = training)
```

These plots show some pretty tight groupings of observations, and a decent visible separation of the outcome values. Yet different outcomes are often close enough together that additional variables beyond these few are likely to be important to the model's performance. The output of `varImp` confirms this.

We can also compare our predicted values with the actual values in our testing data.

```{r}
testing$pred_correct <- predictions == testing$classe
qplot(roll_belt, yaw_belt, color = pred_correct, data = testing)
qplot(roll_belt, magnet_dumbbell_z, color = pred_correct, data = testing)
qplot(yaw_belt, magnet_dumbbell_z, color = pred_correct, data = testing)
```

These plots show which values our model predicted incorrectly in the testing data (pred_correct = FALSE). These seem about what we would expect based on the grouping of data in the previous plots.

### Prediction Submissions

Not technically part of this report, but I need to generate predictions to be submitted for the second part of the project.

```{r}
assignment_testing <- read.csv("./data/pml-testing.csv")
assignment_predictions <- predict(modFit, newdata = assignment_testing)
answers <- as.character(assignment_predictions)

pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(answers)
```
